{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFg0S7BnkPpv"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUu86p1Or1n"
      },
      "source": [
        "# Prediction-Based Word Vectors\n",
        "\n",
        "more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpYg_7pODYJ",
        "outputId": "ee9b4626-d0d1-4d8d-cbd1-42023d7badb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import pprint\n",
        "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfCOLUsSSuS"
      },
      "source": [
        "### Words with Multiple Meanings\n",
        "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
        "\n",
        "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZAr09U-xSSuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559b1cb9-1faf-48b0-9e5f-ef9573bf32ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar words to <head> are : [('heads', 0.7668997645378113), ('headed', 0.6344295144081116), ('chief', 0.6314131617546082), ('body', 0.6098024249076843), ('assistant', 0.6064105033874512), ('director', 0.6037707328796387), ('deputy', 0.5836146473884583), ('hand', 0.5738338232040405), ('left', 0.5574275255203247), ('arm', 0.5565925240516663)]\n"
          ]
        }
      ],
      "source": [
        "### CODE HERE\n",
        "word = \"head\"\n",
        "result = wv_from_bin.most_similar(word)\n",
        "print(f\"The most similar words to <{word}> are : {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQ018tjSSuT"
      },
      "source": [
        "### SOLUTION\n",
        "The word \"head\" can have multiple meanings like:\n",
        "1. brain\n",
        "2. chief teacher\n",
        "3. top of sth\n",
        "4. go towards\n",
        "5. boss of a business\n",
        "6. hit with the head\n",
        "7. title\n",
        "\n",
        "-----------------------------------------------------------------------------\n",
        "\n",
        "Many polysemous or homonymic words might not yield diverse sets of meanings in the top-10 most similar words due to several reasons:\n",
        "\n",
        "1. Frequency and Distribution: The word embeddings are trained based on the distributional properties of words in a corpus. If one meaning of a polysemous word is much more frequent in the training data than others, the embeddings may prioritize that meaning over others, leading to a bias in the similar words retrieved.\n",
        "\n",
        "2. Contextual Ambiguity: Polysemous words often exhibit different meanings depending on the context. Word embeddings capture co-occurrence patterns in the training data, but they might not capture all contextual nuances. As a result, similar words retrieved may be contextually biased towards one meaning over others.\n",
        "\n",
        "3. Semantic Interference: Words with multiple meanings can introduce semantic interference, where the different meanings of the word influence each other's representation in the embeddings. This interference can make it challenging for the model to disambiguate between the meanings effectively.\n",
        "\n",
        "4. Training Data Limitations: The training data might not sufficiently represent all senses or meanings of polysemous words. If certain senses are underrepresented or absent in the training data, the embeddings may not accurately capture the semantic relationships between different meanings of the word.\n",
        "\n",
        "5. Model Limitations: While word embeddings are effective at capturing semantic relationships between words, they have inherent limitations. They represent words as dense vectors in a continuous space, which might not fully capture the complex and multifaceted nature of language, especially when it comes to polysemy and homonymy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeW-eK9SSuU"
      },
      "source": [
        "### Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "As an example, $w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bwlpPjpHSSuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d531f88-3184-4970-e582-8ca9f7b977fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms mother, mama have cosine distance: 0.6049132645130157\n",
            "Antonyms mother, father have cosine distance: 0.20632314682006836\n"
          ]
        }
      ],
      "source": [
        "w1 = \"mother\"\n",
        "w2 = \"mama\"\n",
        "w3 = \"father\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIHjTFMSSuV"
      },
      "source": [
        "### SOLUTION\n",
        "counter-intuitive result may have happened because of:\n",
        "1. Contextual Usage: The Word2Vec model is trained on a large corpus of text and it learns to associate words that are used in similar contexts. If in the training corpus “mother” and “father” appear in similar contexts more often than “mother” and “mama”, the vectors for “mother” and “father” will end up closer together.\n",
        "\n",
        "2. Frequency of Words: “Father” is more commonly used than “mama” in many corpora. Therefore, the model might  have a more accurate representation for “father”, leading to a closer association with “mother”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxIDq26zSSuW"
      },
      "source": [
        "### Analogies with Word Vectors\n",
        "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : grandfather :: woman : x\" (read: man is to grandfather as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u0pC7H4VSSuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbdceac-b9f7-468b-e9cc-186351318442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('grandmother', 0.7608445286750793),\n",
            " ('granddaughter', 0.7200808525085449),\n",
            " ('daughter', 0.7168302536010742),\n",
            " ('mother', 0.7151536345481873),\n",
            " ('niece', 0.7005682587623596),\n",
            " ('father', 0.6659887433052063),\n",
            " ('aunt', 0.6623408794403076),\n",
            " ('grandson', 0.6618767976760864),\n",
            " ('grandparents', 0.644661009311676),\n",
            " ('wife', 0.6445354223251343)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVv8I9WwSSuZ"
      },
      "source": [
        "Let $m$, $g$, $w$, and $x$ denote the word vectors for `man`, `grandfather`, `woman`, and the answer, respectively. Using **only** vectors $m$, $g$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, to what expression are we maximizing $x$'s cosine similarity?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would `man` and `woman` lie in the coordinate plane relative to `grandfather` and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUKBqtHSSuZ"
      },
      "source": [
        "### SOLUTION\n",
        "we have the analogy \"man : grandfather :: woman : x\". First lets break it into vectors:\n",
        "\n",
        "- m:  the vector representing \"man\".\n",
        "- g: the vector representing \"grandfather\".\n",
        "- w: the vector representing \"woman\".\n",
        "- x: the vector representing the unknown word in place of \"x\".\n",
        "\n",
        "Then, we want to find x:\n",
        "1. First we add the vector for \"woman\" to the vector for \"grandfather\", so we will have: g + w\n",
        "2. Then we subtract the vector for \"man\" from the previous: g + w - m\n",
        "\n",
        "This expression represents the direction from \"man\" to \"woman\" applied to the \"grandfather\" vector. In other words, it's the vector representation of the relationship between \"woman\" and \"grandfather\" analogous to the relationship between \"man\" and \"grandfather\".\n",
        "By maximizing the cosine similarity between x and this expression, we aim to find the word most analogous to \"grandfather\" in relation to \"woman\", which is the answer to the analogy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRgMca9SSua"
      },
      "source": [
        "### Finding Analogies\n",
        "a. For the previous example, it's clear that \"grandmother\" completes the analogy. But give an intuitive explanation as to why the `most_similar` function gives us words like \"granddaughter\", \"daughter\", or \"mother?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYQXazQSSua"
      },
      "source": [
        "### SOLUTION\n",
        "The `most_similar` function in Word2Vec or similar word embedding models calculates the cosine similarity between word vectors to find words that are most similar to a given set of words. In the analogy \"man : grandfather :: woman : x\", the function is trying to find the word x that is most similar to the relationship between \"man\" and \"grandfather\" when applied to \"woman\".\n",
        "\n",
        "In this case, \"grandmother\" is the correct answer because it represents the female counterpart of \"grandfather\", which maintains the generational and familial relationship established in the analogy. However, other words like \"granddaughter\", \"daughter\", or \"mother\" are also retrieved because they share certain semantic similarities or associations with the given words.\n",
        "\n",
        "To investigate\"\n",
        "1. Granddaughter: It represents a familial relationship, and while it's not exactly the same as \"grandfather\", it's still within the familial hierarchy.\n",
        "2. Daughter: This is a direct familial relationship with \"woman\", but it's not as directly linked to \"grandfather\" as \"grandmother\" would be.\n",
        "3. Mother: Again, a direct familial relationship, but in this case, it's one step further removed from \"grandfather\" compared to \"grandmother\".\n",
        "\n",
        "These words appear in the results because they share semantic relationships with the given words \"man\", \"woman\", and \"grandfather\", but \"grandmother\" is the closest match in terms of the specific familial relationship being described in the analogy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAUXEISSub"
      },
      "source": [
        "b. Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "**Note**: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dhzQJMYYVSjf"
      },
      "outputs": [],
      "source": [
        "x, y, a, b = 'paris', 'france', 'beijing', 'china'\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] == b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QlPqAwSSub"
      },
      "source": [
        "### SOLUTION\n",
        "The analogy is: \"Paris : France :: Beijing : China.\"\n",
        "\n",
        "This analogy holds because both Paris and Beijing are capital cities, and they each correspond to their respective countries, France and China. So, the relationship between Paris and France is analogous to the relationship between Beijing and China."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwgcEywwSSuc"
      },
      "source": [
        "### Incorrect Analogy\n",
        "a. Below, we expect to see the intended analogy \"hand : glove :: foot : **sock**\", but we see an unexpected result instead. Give a potential reason as to why this particular analogy turned out the way it did?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m-ykWoJoSSuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9c16e2-ce16-483f-e4ec-d694df7f96f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('45,000-square', 0.4922032654285431),\n",
            " ('15,000-square', 0.4649604558944702),\n",
            " ('10,000-square', 0.4544755816459656),\n",
            " ('6,000-square', 0.44975775480270386),\n",
            " ('3,500-square', 0.444133460521698),\n",
            " ('700-square', 0.44257497787475586),\n",
            " ('50,000-square', 0.4356396794319153),\n",
            " ('3,000-square', 0.43486514687538147),\n",
            " ('30,000-square', 0.4330596923828125),\n",
            " ('footed', 0.43236875534057617)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4ruS8MSSud"
      },
      "source": [
        "### SOLUTION\n",
        "here the problem is that the foot is considered as a metric of calculation of distance, not a part of body."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gHyZt0SSud"
      },
      "source": [
        "b. Find another example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the **incorrect** value of b according to the word vectors (in the previous example, this would be **'45,000-square'**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D_rlci42XQTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9682c5c3-3ada-4e0c-e275-5968acb0650a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('chips', 0.5595942735671997),\n",
            " ('ibm', 0.5587440133094788),\n",
            " ('chip', 0.5563934445381165),\n",
            " ('intel', 0.5496072769165039),\n",
            " ('sony', 0.5326699614524841),\n",
            " ('macintosh', 0.5229740738868713),\n",
            " ('microsoft', 0.5217815637588501),\n",
            " ('iphone', 0.5164424777030945),\n",
            " ('hewlett', 0.5043824315071106),\n",
            " ('itunes', 0.5014412999153137)]\n"
          ]
        }
      ],
      "source": [
        "x, y, a, b = \"red\", \"blue\", \"apple\", \"sky\"\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0EHjeSSue"
      },
      "source": [
        "### SOLUTION\n",
        "Here the analogy is: \"red:apple :: blue:sky.\"\n",
        "\n",
        "And here again the prediction is not as we considered because the word vector considers apple as a company brand rather than a fruit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlycXN-SSuf"
      },
      "source": [
        "### Guided Analysis of Bias in Word Vectors\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XggWA4MhSSuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7342a118-8bb8-41ae-a342-9bf6ebe439db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reputation', 0.5250176787376404),\n",
            " ('professions', 0.5178037881851196),\n",
            " ('skill', 0.49046966433525085),\n",
            " ('skills', 0.49005505442619324),\n",
            " ('ethic', 0.4897659420967102),\n",
            " ('business', 0.4875852167606354),\n",
            " ('respected', 0.485920250415802),\n",
            " ('practice', 0.482104629278183),\n",
            " ('regarded', 0.4778572618961334),\n",
            " ('life', 0.4760662019252777)]\n",
            "\n",
            "[('professions', 0.5957457423210144),\n",
            " ('practitioner', 0.49884122610092163),\n",
            " ('teaching', 0.48292139172554016),\n",
            " ('nursing', 0.48211804032325745),\n",
            " ('vocation', 0.4788965880870819),\n",
            " ('teacher', 0.47160351276397705),\n",
            " ('practicing', 0.46937814354896545),\n",
            " ('educator', 0.46524327993392944),\n",
            " ('physicians', 0.4628995358943939),\n",
            " ('professionals', 0.4601394236087799)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be most dissimilar from.\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4g6KbsYSSuh"
      },
      "source": [
        "### SOLUTION\n",
        "we can observe the gender bias inherent in word embeddings:\n",
        "1. For terms similar to \"man\" and \"profession\" while being dissimilar to \"woman\", we see words such as \"reputation\", \"skill\", \"business\", \"respected\", and \"regarded\". These terms are more abstract and less directly related to specific professions. This result suggests that the model associates \"man\" with qualities like reputation, skill, and being respected in professional settings rather than specific professions themselves.\n",
        "2. For terms similar to \"woman\" and \"profession\" while being dissimilar to \"man\", we see words like \"nursing\", \"teaching\", \"teacher\", \"educator\", and \"physicians\". These terms are more specific and directly related to certain professions. This result suggests that the model associates \"woman\" more with specific professions like nursing and teaching rather than abstract qualities or a diverse range of professions.\n",
        "\n",
        "The difference between the lists reflects gender bias in societal perceptions of professions. Historically, certain professions like nursing and teaching have been associated more with women, while others like business and medicine have been associated more with men. This bias gets reflected in the word embeddings due to the patterns present in the training data, perpetuating stereotypes and potentially reinforcing societal biases when used in applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJmnS6lSSui"
      },
      "source": [
        "### Independent Analysis of Bias in Word Vectors\n",
        "\n",
        "Use the `most_similar` function to find another pair of analogies that demonstrates some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PZoDheIfSSui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481368f8-48f6-4300-c8ed-fa23b943a2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('physician', 0.6719361543655396),\n",
            " ('surgeon', 0.6208168268203735),\n",
            " ('dr.', 0.5724585056304932),\n",
            " ('brother', 0.5710500478744507),\n",
            " ('son', 0.5303334593772888),\n",
            " ('he', 0.5294877290725708),\n",
            " ('medical', 0.528836190700531),\n",
            " ('uncle', 0.5231919884681702),\n",
            " ('himself', 0.5133481621742249),\n",
            " ('pharmacist', 0.5111744403839111)]\n",
            "\n",
            "[('nurse', 0.7208659648895264),\n",
            " ('doctors', 0.6413154602050781),\n",
            " ('patient', 0.6289440393447876),\n",
            " ('woman', 0.6113752126693726),\n",
            " ('hospital', 0.6000143885612488),\n",
            " ('pregnant', 0.5975667238235474),\n",
            " ('nurses', 0.572587788105011),\n",
            " ('physician', 0.5669365525245667),\n",
            " ('medical', 0.5617853403091431),\n",
            " ('patients', 0.5472391843795776)]\n"
          ]
        }
      ],
      "source": [
        "A = 'father'\n",
        "B = 'mother'\n",
        "word = 'doctor'\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOlmtJoSSuj"
      },
      "source": [
        "### SOLUTION\n",
        "we can observe bias in the associations of the word \"doctor\" with \"father\" and \"mother\":\n",
        "- When exploring the association between \"doctor\" and \"father\", the top similar words include \"physician\", \"surgeon\", \"dr.\", and \"brother\". These terms are all related to the medical profession, indicating a strong association between \"doctor\" and male family roles.\n",
        "- when exploring the association between \"doctor\" and \"mother\", the top similar words include \"nurse\", \"doctors\", \"patient\", and \"woman\". Here, \"nurse\" stands out as the most prominent term, reflecting a bias in the model associating women more with nursing roles rather than being doctors.\n",
        "\n",
        "This example reflects the societal bias that traditionally portrays men as more likely to be doctors while women are more likely to be nurses, perpetuating gender stereotypes in professional roles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2XVWzmSSuk"
      },
      "source": [
        "### Thinking About Bias\n",
        "\n",
        "a. Give one explanation of how bias gets into the word vectors. Briefly describe a real-world example that demonstrates this source of bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pM85fCSSuk"
      },
      "source": [
        "### SOLUTION\n",
        "One explanation of how bias gets into word vectors is through the biases present in the training data used to train the models. Word embedding models like Word2Vec are trained on large corpora of text, which reflects the language usage patterns present in the data. If the training data contains biases or reflects societal stereotypes, these biases get encoded into the word vectors.\n",
        "\n",
        "A real-world example of this source of bias can be seen in the representation of gender roles in professions. If the training data predominantly contains examples where men are associated with certain professions like \"doctor\" or \"engineer\", while women are associated with others like \"nurse\" or \"teacher\", the resulting word vectors will reflect and reinforce these stereotypes. This can perpetuate gender biases in applications that utilize these word embeddings, such as natural language processing systems or recommendation algorithms, potentially leading to biased outcomes or reinforcing societal inequalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYqJZ7ASSul"
      },
      "source": [
        "b. What is one method you can use to mitigate bias exhibited by word vectors?  Briefly describe a real-world example that demonstrates this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJaAB7mSSul"
      },
      "source": [
        "\n",
        "### SOLUTION\n",
        "One method to mitigate bias exhibited by word vectors is through debiasing techniques during or after the training process. This involves identifying and neutralizing biased associations present in the word vectors.\n",
        "\n",
        "One common debiasing method is to identify gender-specific biases and neutralize them by projecting gender-neutral vectors. For example, in the context of professions, if the word vector for \"nurse\" is closer to female gender words and \"doctor\" is closer to male gender words, a debiasing algorithm could adjust the word vectors such that the gender associations are minimized.\n",
        "\n",
        "A real-world example demonstrating this method is the work by Bolukbasi et al. (2016) titled \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\". They proposed a method to debias word embeddings by neutralizing gender-specific associations in word vectors. By applying this method, they were able to mitigate gender bias in word embeddings, reducing the association between gender-neutral professions and gender-specific terms.\n",
        "Also, this debiased word vector can be used in various Natural Language Processing (NLP) tasks such as machine translation, sentiment analysis, or information retrieval, helping to ensure that the outcomes of these tasks are less biased. For example, a job recommendation system using debiased word vectors would be less likely to show gender-stereotyped job ads to users."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}